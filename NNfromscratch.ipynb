{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "#just testing what they use in matplotlib\n",
    "np.arange(0,20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([ 3, 10]),\n array([1, 2, 3, 4]),\n array([[1, 1],\n        [1, 1],\n        [0, 1],\n        [0, 1]]))"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([ [1 for i in range(4)],[(0 if i%2==0 else 1) for i in range(4)] ]).reshape(4,2)\n",
    "a @ b, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1. , 2. , 3. , 2.5]])"
     },
     "metadata": {},
     "execution_count": 561
    }
   ],
   "source": [
    "x = np.array([1,2,3,2.5]).reshape(1,x.shape[1])\n",
    "#weights = [ [.2,.8,-0.5,1],[0.5,-0.91,.26,-.5],[-0.26,-0.27,.17,.87] ]\n",
    "biases = [2,3,.5]\n",
    "y = np.array(1).reshape(1,1)\n",
    "\n",
    "theta = np.random.rand(1,3)\n",
    "theta\n",
    "\n",
    "# matrix for generalisation \n",
    "x\n",
    "\n",
    "#np.dot will convert list to arrays before multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[0.2 , 0.8 , 0.  , 1.  ],\n        [0.5 , 0.  , 0.26, 0.  ],\n        [0.  , 0.  , 0.17, 0.87]]),\n array([[ 0.2 ,  0.8 , -0.5 ,  1.  ],\n        [ 0.5 , -0.91,  0.26, -0.5 ],\n        [-0.26, -0.27,  0.17,  0.87]]))"
     },
     "metadata": {},
     "execution_count": 67
    },
    {
     "output_type": "stream",
     "text": "\u001b[1;31mCall signature:\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mType:\u001b[0m            ufunc\n\u001b[1;31mString form:\u001b[0m     <ufunc 'maximum'>\n\u001b[1;31mFile:\u001b[0m            c:\\users\\zouru\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\n\u001b[1;31mDocstring:\u001b[0m      \nmaximum(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj])\n\nElement-wise maximum of array elements.\n\nCompare two arrays and returns a new array containing the element-wise\nmaxima. If one of the elements being compared is a NaN, then that\nelement is returned. If both elements are NaNs then the first is\nreturned. The latter distinction is important for complex NaNs, which\nare defined as at least one of the real or imaginary parts being a NaN.\nThe net effect is that NaNs are propagated.\n\nParameters\n----------\nx1, x2 : array_like\n    The arrays holding the elements to be compared. If ``x1.shape != x2.shape``, they must be broadcastable to a common shape (which becomes the shape of the output).\nout : ndarray, None, or tuple of ndarray and None, optional\n    A location into which the result is stored. If provided, it must have\n    a shape that the inputs broadcast to. If not provided or None,\n    a freshly-allocated array is returned. A tuple (possible only as a\n    keyword argument) must have length equal to the number of outputs.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the\n    :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\ny : ndarray or scalar\n    The maximum of `x1` and `x2`, element-wise.\n    This is a scalar if both `x1` and `x2` are scalars.\n\nSee Also\n--------\nminimum :\n    Element-wise minimum of two arrays, propagates NaNs.\nfmax :\n    Element-wise maximum of two arrays, ignores NaNs.\namax :\n    The maximum value of an array along a given axis, propagates NaNs.\nnanmax :\n    The maximum value of an array along a given axis, ignores NaNs.\n\nfmin, amin, nanmin\n\nNotes\n-----\nThe maximum is equivalent to ``np.where(x1 >= x2, x1, x2)`` when\nneither x1 nor x2 are nans, but it is faster and does proper\nbroadcasting.\n\nExamples\n--------\n>>> np.maximum([2, 3, 4], [1, 5, 2])\narray([2, 5, 4])\n\n>>> np.maximum(np.eye(2), [0.5, 2]) # broadcasting\narray([[ 1. ,  2. ],\n       [ 0.5,  2. ]])\n\n>>> np.maximum([np.nan, 0, np.nan], [0, np.nan, np.nan])\narray([nan, nan, nan])\n>>> np.maximum(np.Inf, 1)\ninf\n\u001b[1;31mClass docstring:\u001b[0m\nFunctions that operate element by element on whole arrays.\n\nTo see the documentation for a specific ufunc, use `info`.  For\nexample, ``np.info(np.sin)``.  Because ufuncs are written in C\n(for speed) and linked into Python with NumPy's ufunc facility,\nPython's help() function finds this page whenever help() is called\non a ufunc.\n\nA detailed explanation of ufuncs can be found in the docs for :ref:`ufuncs`.\n\nCalling ufuncs:\n===============\n\nop(*x[, out], where=True, **kwargs)\nApply `op` to the arguments `*x` elementwise, broadcasting the arguments.\n\nThe broadcasting rules are:\n\n* Dimensions of length 1 may be prepended to either array.\n* Arrays may be repeated along dimensions of length 1.\n\nParameters\n----------\n*x : array_like\n    Input arrays.\nout : ndarray, None, or tuple of ndarray and None, optional\n    Alternate array object(s) in which to put the result; if provided, it\n    must have a shape that the inputs broadcast to. A tuple of arrays\n    (possible only as a keyword argument) must have length equal to the\n    number of outputs; use None for uninitialized outputs to be\n    allocated by the ufunc.\nwhere : array_like, optional\n    This condition is broadcast over the input. At locations where the\n    condition is True, the `out` array will be set to the ufunc result.\n    Elsewhere, the `out` array will retain its original value.\n    Note that if an uninitialized `out` array is created via the default\n    ``out=None``, locations within it where the condition is False will\n    remain uninitialized.\n**kwargs\n    For other keyword-only arguments, see the :ref:`ufunc docs <ufuncs.kwargs>`.\n\nReturns\n-------\nr : ndarray or tuple of ndarray\n    `r` will have the shape that the arrays in `x` broadcast to; if `out` is\n    provided, it will be returned. If not, `r` will be allocated and\n    may contain uninitialized values. If the function has more than one\n    output, then the result will be a tuple of arrays.\n"
    }
   ],
   "source": [
    "?np.maximum\n",
    "# append after comparing maximum between 0 and 1 for each element\n",
    "np.maximum(0,weights),np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0.23253709]],shape : (1, 1)\n"
    }
   ],
   "source": [
    "#debugger\n",
    "def printer(*args):\n",
    "    all = ''\n",
    "    for i in args:\n",
    "        all += f\"{i},shape : {i.shape if isinstance(i,np.ndarray) else 'this is not np.array'}\"\n",
    "    print(all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dense layer is just another regular layer in a neural network\n",
    "\n",
    "def a_h(x,weight):\n",
    "    return x @ weight.T\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def nn_size(m,n_neurons):\n",
    "    return np.random.rand( m ,n_neurons)\n",
    "    \n",
    "\n",
    "#all kept within a single function to reuse variables & values\n",
    "\n",
    "#instantiate random weights\n",
    "\n",
    "\n",
    "w0 = np.array(np.random.rand(1,4))\n",
    "' neural network weights dim is  no. features * no. neurons next layer '\n",
    "weights = nn_size( x.shape[1] ,4)\n",
    "\n",
    "\n",
    "def forward(x,n_neurons_next,weights,*w0):\n",
    "    #no. of entries x no. of features\n",
    "    m,n = x.shape[0],x.shape[1]\n",
    "\n",
    "    # include bias\n",
    "    bias = np.ones(( m,n_neurons_next )).reshape( m,n_neurons_next )\n",
    "    \n",
    "    #accounting for first layer missing w0, function reuse in backprop\n",
    "    ' size should be x.shape[1] which is number of neurons in existing layer to fit bias to every single neuron '\n",
    "\n",
    "    if not w0:\n",
    "        w0 = np.random.rand(1,n_neurons_next)\n",
    "    else:\n",
    "        w0=w0[0]\n",
    "\n",
    "    \n",
    "\n",
    "    # activation of the next layer, there are n_neurons , thus n_neuron activations (h(x))\n",
    "    #relu it instead of sigmoid\n",
    "    #print('bias and w0'),printer(bias,w0)\n",
    "\n",
    "    a_h(bias, w0)\n",
    "    \n",
    "    a1 =  sigmoid( a_h(x,weights)   )\n",
    "\n",
    "\n",
    "    #weights including bias\n",
    "\n",
    "\n",
    "    ' correct,  multiplying one theta0 weight (by column) per row'\n",
    "    weights = np.vstack( (w0,weights.reshape(m,n_neurons_next)) )\n",
    "    ' weights dim n*nn+1 ; n is number of features , nn is number of neurons per layer'\n",
    "\n",
    "    #print(f'a1 is {a1}!!!')\n",
    "    return a1,weights\n",
    "\n",
    "\n",
    "def a_Relu(inputs):\n",
    "    return np.maximum(0,inputs)\n",
    "\n",
    "# optimiser Adam, Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "lda3\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[-0.23666636]]), array([[-0.05591689, -0.04280603]]))"
     },
     "metadata": {},
     "execution_count": 602
    }
   ],
   "source": [
    "gradDesc(x,y,weights,0.001,100,[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1.  2.  3.  2.5]],shape : (1, 4)[[0.04157263 0.36032791 0.55802922 0.35554947]\n [0.60620643 0.14641091 0.03041007 0.12524438]\n [0.30373598 0.59231388 0.89458624 0.42280472]\n [0.72209978 0.99760761 0.40823771 0.88956193]],shape : (4, 4)\n[[0.96528293 0.7864015  0.99467039 0.99790464]],shape : (1, 4)[[0.27924801 0.07293734 0.63204093 0.40579998]\n [0.04157263 0.36032791 0.55802922 0.35554947]\n [0.60620643 0.14641091 0.03041007 0.12524438]\n [0.30373598 0.59231388 0.89458624 0.42280472]\n [0.72209978 0.99760761 0.40823771 0.88956193]],shape : (5, 4)\n"
    }
   ],
   "source": [
    "#nn_size(4,4)\n",
    "#print(w0.shape,'w0',x.shape[1])\n",
    "#print( np.hstack( (w0.reshape(1,1),weights) ) )\n",
    "\n",
    "a1,w1 = forward(x,4,weights,w0)\n",
    "#returns layer 1 activation and weights theta(1)\n",
    "printer(a1,w1)\n",
    "# hypothesis for layer 1, 4 neurons\n",
    "# dim should be m*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f'weight is {weights}') \n",
    "\n",
    "def cost(x,y,weights):\n",
    "    m = x.shape[0]\n",
    "    return 1/m * np.sum( np.sum( y* np.log( sigmoid(a_h(x,weights))) + (1-y)*np.log(1-sigmoid (a_h(x,weights)) ) ) )\n",
    "\n",
    "\n",
    "def gradDesc(x,y,weights,alpha,iter,layers):\n",
    "    m,n = x.shape[0], x.shape[1]\n",
    "\n",
    "    # including bias\n",
    "    ' weights param must include weights for bias unit ! '\n",
    "\n",
    "    n1,n2= layers[0],layers[1]\n",
    "\n",
    "    # activation and weights with bias weights\n",
    "    a1,w1 = forward(x,n1,nn_size(m,n1))\n",
    "\n",
    "    # weights supposed to be (no. of features * no. neurons next layer)\n",
    "    a2,w2= forward(a1, n2 , nn_size(n2,n2) )\n",
    "    'nn_size(a,b) , whereby a=no. of features in existing layer, b is no. of features in next' \n",
    "\n",
    "    # backprop\n",
    "    print('lda3')\n",
    "    lda3 = a2 -y\n",
    "    lda2 = a_h(lda3,w2)*( a2*(1-a2) )   #broadcast\n",
    "\n",
    "    #merge into matrix\n",
    "\n",
    "    # set deriv to 0\n",
    "    djd0 = 0\n",
    "    djdi = 0\n",
    "\n",
    "\n",
    "    #all the derivatives\n",
    "    djd0 = a_h(a2,lda3) \n",
    "    djdi = a_h(a1,lda2.T)\n",
    "\n",
    "    ''' a for loop with number of iter is to accumulate terms on each iter and perform gradient descent \n",
    "    '''\n",
    "\n",
    "    # djd0 = 1/m * djd0\n",
    "    # djdi = 1/m * djdi\n",
    "\n",
    "    return djd0,djdi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[4.72080193]]), array([[nan]]), array([[nan]]))"
     },
     "metadata": {},
     "execution_count": 207
    }
   ],
   "source": [
    "a_h(x,weights), ( (1-y) * np.log( 1-a_Relu(a_h(x,weights)) )), a_h(x,weights) + ( (1-y) * np.log( 1-a_Relu(a_h(x,weights)) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "weights\n[[0.85219858 0.94004272 0.49363345 0.61497112]],shape : (1, 4)[[0.00727035 0.84417149 0.42559869 0.94652051]],shape : (1, 4)\n m is 1 nn is 4,weight shape is (2, 4) weights is [[0.85219858 0.94004272 0.49363345 0.61497112]\n [0.00727035 0.84417149 0.42559869 0.94652051]]\nweights\n[[0.60493919]],shape : (1, 1)[[0.96565247]],shape : (1, 1)\n m is 1 nn is 1,weight shape is (2, 1) weights is [[0.60493919]\n [0.96565247]]\nlda3\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[-0.20012394]]), array([[-0.03333444, -0.0532111 ]]))"
     },
     "metadata": {},
     "execution_count": 592
    }
   ],
   "source": [
    "gradDesc(x,y,weights,0.001,100,[4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(array([[1. , 2. , 3. , 2.5]]),\n array([[1]]),\n array([[0.04157263, 0.36032791, 0.55802922, 0.35554947],\n        [0.60620643, 0.14641091, 0.03041007, 0.12524438],\n        [0.30373598, 0.59231388, 0.89458624, 0.42280472],\n        [0.72209978, 0.99760761, 0.40823771, 0.88956193]]))"
     },
     "metadata": {},
     "execution_count": 454
    }
   ],
   "source": [
    "x,y,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}